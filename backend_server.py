"""
Driver Behavior Detection Backend API Server
- ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì› (ì„¸ì…˜ë³„ ë²„í¼ ê´€ë¦¬)
- ë°°ì¹˜ ì¶”ë¡ ìœ¼ë¡œ GPU íš¨ìœ¨ ìµœëŒ€í™”
- WebSocketìœ¼ë¡œ ì‹¤ì‹œê°„ í†µì‹ 
- SQLite ê¸°ë°˜ ì‚¬ìš©ì ì¸ì¦ (ì‹œì—°ìš©)
- GPU ìƒì‹œ ëŒ€ê¸° + ì¦‰ì‹œ ë°°ì¹˜ ì²˜ë¦¬
- ìµœì í™”: torch.compile, CUDA Streams, Pinned Memory
"""
import os
import sys

# CUDA ë©”ëª¨ë¦¬ ë‹¨í¸í™” ë°©ì§€ (torch import ì „ì— ì„¤ì •)
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'

import json
import base64
import asyncio
import numpy as np
import torch
import time
import uuid
import sqlite3
from collections import defaultdict
from io import BytesIO
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from pydantic import BaseModel
from typing import Optional, Dict, Any, List
import uvicorn
import threading
from queue import Queue, Empty
from dataclasses import dataclass

# ëª¨ë¸ ë¡œë“œ
sys.path.insert(0, '/root/Driver_monitoring')
from model import DriverBehaviorModel

# í´ë˜ìŠ¤ ì •ì˜
CLASS_NAMES = {
    0: "Normal",
    1: "Drowsy",
    2: "Searching",
    3: "Phone",
    4: "Assault"
}

# ì „ì—­ ë³€ìˆ˜
model = None
compiled_model = None  # torch.compile ìµœì í™” ëª¨ë¸
device = None
preallocated_buffer = None  # GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í™•ë³´ìš©

# GPU ì •ê·œí™” ìƒìˆ˜ (CUDA í…ì„œ)
gpu_mean = None
gpu_std = None

# CUDA Streams (ë¹„ë™ê¸° ì²˜ë¦¬ìš©)
inference_stream = None
transfer_stream = None

# Pinned Memory ë²„í¼ (CPU-GPU ì „ì†¡ ìµœì í™”)
pinned_buffer = None

# ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ (ìŠ¤ë ˆë“œê°„ WebSocket í†µì‹ ìš©)
main_event_loop = None

# ë‹¤ì¤‘ ì‚¬ìš©ì ì„¸ì…˜ ê´€ë¦¬
user_sessions: Dict[str, Dict] = {}
sessions_lock = threading.Lock()

# ë°°ì¹˜ ì¶”ë¡  ì„¤ì • (GPU ìµœì í™” - Video Swin Transformer ë©”ëª¨ë¦¬ ê³ ë ¤)
BATCH_SIZE = 16  # 8 -> 16 (ì²˜ë¦¬ëŸ‰ 2ë°° ì¦ê°€, OOM ë°©ì§€)
FRAMES_PER_INFERENCE = 30
FRAME_BUFFER_SIZE = 60  # ë²„í¼ í¬ê¸° (60í”„ë ˆì„ ëª¨ìœ¼ê³ )
FRAME_SHIFT = 10  # ì¶”ë¡  í›„ ì‹œí”„íŠ¸ëŸ‰ (10í”„ë ˆì„ì”© ì´ë™ = 33% ìƒˆ ë°ì´í„°)
BATCH_TIMEOUT = 0.1  # 50ms -> 100ms (ë°°ì¹˜ ì±„ìš°ê¸° ì‹œê°„ ì¦ê°€)

# ì¶”ë¡  í (ì¦‰ì‹œ ì²˜ë¦¬ìš©)
@dataclass
class InferenceJob:
    session_id: str
    frames: List[np.ndarray]
    websocket: Optional[any] = None
    timestamp: float = 0.0

inference_queue: Queue = Queue()
results_store: Dict[str, Dict] = {}  # HTTP í´ë§ìš© ê²°ê³¼ ì €ì¥

# ë””ë²„ê·¸ìš© í”„ë ˆì„ ì €ì¥
DEBUG_DIR = "/tmp/inference_debug"
os.makedirs(DEBUG_DIR, exist_ok=True)
inference_history: List[Dict] = []  # ìµœê·¼ ì¶”ë¡  ê¸°ë¡ (ìµœëŒ€ 100ê°œ)
SAVE_DEBUG_FRAMES = False  # í”„ë¡œë•ì…˜ì—ì„œëŠ” ë¹„í™œì„±í™” (ì„±ëŠ¥ ìµœì í™”)

# ë¹„ë™ê¸° ë””ë²„ê·¸ ì €ì¥ìš© ìŠ¤ë ˆë“œ í’€
from concurrent.futures import ThreadPoolExecutor
debug_executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="debug_saver")

# SQLite DB ì„¤ì •
DB_PATH = '/root/users.db'

def init_database():
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()

    # ì‚¬ìš©ì í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS users (
            id TEXT PRIMARY KEY,
            name TEXT NOT NULL,
            password TEXT NOT NULL,
            address TEXT,
            region_name TEXT,
            region_campaign TEXT,
            region_target INTEGER DEFAULT 90,
            region_reward TEXT,
            score INTEGER DEFAULT 80,
            discount_rate INTEGER DEFAULT 0,
            created_at TEXT
        )
    ''')

    # ìš´ì „ ê¸°ë¡ í…Œì´ë¸” (ì„¸ì…˜ ì •ë³´)
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS driving_logs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            user_id TEXT NOT NULL,
            start_time TEXT NOT NULL,
            end_time TEXT,
            status TEXT DEFAULT 'driving',
            total_detections INTEGER DEFAULT 0,
            normal_count INTEGER DEFAULT 0,
            drowsy_count INTEGER DEFAULT 0,
            searching_count INTEGER DEFAULT 0,
            phone_count INTEGER DEFAULT 0,
            assault_count INTEGER DEFAULT 0,
            FOREIGN KEY (user_id) REFERENCES users(id)
        )
    ''')

    # ê°œë³„ ê°ì§€ ê¸°ë¡ í…Œì´ë¸”
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS driving_detections (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            driving_log_id INTEGER NOT NULL,
            detected_at TEXT NOT NULL,
            class_id INTEGER NOT NULL,
            class_name TEXT NOT NULL,
            confidence REAL NOT NULL,
            FOREIGN KEY (driving_log_id) REFERENCES driving_logs(id)
        )
    ''')

    conn.commit()
    conn.close()
    print("Database initialized!")

def get_db_connection():
    conn = sqlite3.connect(DB_PATH)
    conn.row_factory = sqlite3.Row
    return conn

def load_model():
    """ëª¨ë¸ ë¡œë“œ ë° GPU ì›Œë°ì—… + ë©”ëª¨ë¦¬ ì‚¬ì „ í™•ë³´ + ìµœì í™”"""
    global model, compiled_model, device, preallocated_buffer, gpu_mean, gpu_std
    global inference_stream, transfer_stream, pinned_buffer

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # ===== ìµœì í™” 1: cuDNN ë²¤ì¹˜ë§ˆí¬ í™œì„±í™” =====
    if torch.cuda.is_available():
        torch.backends.cudnn.benchmark = True  # ì…ë ¥ í¬ê¸° ê³ ì •ì‹œ ìµœì  ì•Œê³ ë¦¬ì¦˜ ìë™ ì„ íƒ
        torch.backends.cudnn.deterministic = False  # ì•½ê°„ì˜ ë¹„ê²°ì •ì„± í—ˆìš© (ì†ë„â†‘)
        torch.backends.cuda.matmul.allow_tf32 = True  # TF32 ì‚¬ìš© (Ada GPU ìµœì í™”)
        torch.backends.cudnn.allow_tf32 = True
        print("âœ… cuDNN benchmark + TF32 enabled")

    # ===== ìµœì í™” 2: CUDA Streams ìƒì„± =====
    if torch.cuda.is_available():
        inference_stream = torch.cuda.Stream()
        transfer_stream = torch.cuda.Stream()
        print("âœ… CUDA Streams created (async transfer + inference)")

    # GPU ì •ê·œí™” ìƒìˆ˜ ì´ˆê¸°í™”
    gpu_mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)
    gpu_std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)

    if torch.cuda.is_available():
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU Memory: {gpu_mem:.1f} GB")

    model = DriverBehaviorModel(num_classes=5, pretrained=False)
    checkpoint = torch.load('/root/Driver_monitoring/pytorch_model.bin',
                           map_location='cpu', weights_only=True)
    # ìƒˆ í˜•ì‹: {'model': state_dict} / êµ¬ í˜•ì‹: state_dict ì§ì ‘
    state_dict = checkpoint.get('model', checkpoint) if isinstance(checkpoint, dict) else checkpoint
    model.load_state_dict(state_dict)
    model = model.to(device)
    model.eval()

    # ===== ìµœì í™” 3: torch.compile ì ìš© (ì„ íƒì ) =====
    # torch.compileì€ ì²« ì¶”ë¡ ì— ë„ˆë¬´ ì˜¤ë˜ ê±¸ë ¤ì„œ WebSocket íƒ€ì„ì•„ì›ƒ ìœ ë°œ
    # í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ì „ ì›Œë°ì—… ì™„ë£Œ í›„ í™œì„±í™” ê¶Œì¥
    USE_TORCH_COMPILE = False
    if USE_TORCH_COMPILE:
        print("ğŸ”§ Applying torch.compile (reduce-overhead mode)...")
        try:
            compiled_model = torch.compile(
                model,
                mode="reduce-overhead",  # ì§€ì—°ì‹œê°„ ìµœì†Œí™” ëª¨ë“œ
                fullgraph=False,  # ë™ì  shape í—ˆìš©
                dynamic=False,  # ì •ì  shape (224x224 ê³ ì •)
            )
            print("âœ… torch.compile applied successfully")
        except Exception as e:
            print(f"âš ï¸ torch.compile failed, using original model: {e}")
            compiled_model = model
    else:
        compiled_model = model
        print("â„¹ï¸ Using original model (torch.compile disabled)")

    # ===== ìµœì í™” 4: Pinned Memory í• ë‹¹ =====
    if torch.cuda.is_available():
        # CPUâ†’GPU ì „ì†¡ ìµœì í™”ìš© ê³ ì • ë©”ëª¨ë¦¬
        pinned_buffer = torch.empty(
            BATCH_SIZE, 3, 30, 224, 224,
            dtype=torch.float32,
            pin_memory=True
        )
        print("âœ… Pinned memory buffer allocated")

    # GPU ì›Œë°ì—… - ìµœëŒ€ ë°°ì¹˜ë¡œ ë”ë¯¸ ì¶”ë¡  (compiled_model ì‚¬ìš©)
    print("ğŸ”¥ Warming up compiled model with max batch...")
    with torch.no_grad():
        dummy_input = torch.randn(BATCH_SIZE, 3, 30, 224, 224, device=device)
        # torch.compile ì›Œë°ì—… (ì²« ëª‡ ë²ˆì€ ì»´íŒŒì¼ ì˜¤ë²„í—¤ë“œ)
        for i in range(10):  # ì¶©ë¶„íˆ ì›Œë°ì—…
            _ = compiled_model(dummy_input)
        torch.cuda.synchronize()
    print("âœ… Model warmup complete")

    # GPU ë©”ëª¨ë¦¬ ì‚¬ì „ í™•ë³´ - ì‹¤ì œ ì¶”ë¡ ì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•´ì„œ ë©”ëª¨ë¦¬ í’€ í™•ì¥
    print("Pre-allocating GPU memory for max throughput...")

    # ì‹¤ì œ max batch ì¶”ë¡ ì„ ì—¬ëŸ¬ ë²ˆ ìˆ˜í–‰í•˜ì—¬ CUDA ë©”ëª¨ë¦¬ í’€ í™•ì¥
    with torch.no_grad():
        for i in range(10):
            test_input = torch.randn(BATCH_SIZE, 3, 30, 224, 224, device=device)
            _ = compiled_model(test_input)
        torch.cuda.synchronize()  # ë§ˆì§€ë§‰ì—ë§Œ ë™ê¸°í™”

    # ë©”ëª¨ë¦¬ ìƒíƒœ ì¶œë ¥
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        max_allocated = torch.cuda.max_memory_allocated() / 1024**3
        print(f"GPU Memory - Current: {allocated:.2f} GB, Reserved: {reserved:.2f} GB, Peak: {max_allocated:.2f} GB")

    # ì…ë ¥ ë²„í¼ ìœ ì§€ (GC ë°©ì§€)
    preallocated_buffer = {
        'input': torch.zeros(BATCH_SIZE, 3, 30, 224, 224, device=device),
    }
    torch.cuda.synchronize()

    print(f"Model ready! Max batch size: {BATCH_SIZE}")
    return model

# FastAPI ì•±
app = FastAPI(title="Driver Behavior Detection API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ëª¨ë¸
class InferRequest(BaseModel):
    session_id: str
    image: str

class SignUpRequest(BaseModel):
    id: str
    name: str
    password: str
    address: Optional[str] = None
    region_name: Optional[str] = "ì „êµ­ ê³µí†µ"
    region_campaign: Optional[str] = "ëŒ€í•œë¯¼êµ­ ì•ˆì „ìš´ì „ ì±Œë¦°ì§€"
    region_target: Optional[int] = 90
    region_reward: Optional[str] = "ì•ˆì „ìš´ì „ ì¸ì¦ì„œ ë°œê¸‰"

class LoginRequest(BaseModel):
    id: str
    password: str

class StartDrivingRequest(BaseModel):
    user_id: str

class EndDrivingRequest(BaseModel):
    driving_log_id: int

class SaveDetectionRequest(BaseModel):
    driving_log_id: int
    class_id: int
    class_name: str
    confidence: float

# í™œì„± ìš´ì „ ì„¸ì…˜ ë§¤í•‘ (session_id -> driving_log_id)
active_driving_logs: Dict[str, int] = {}

def preprocess_image(base64_image: str) -> np.ndarray:
    """Base64 ì´ë¯¸ì§€ ë””ì½”ë”© + ë¦¬ì‚¬ì´ì¦ˆ (ì •ê·œí™”ëŠ” GPUì—ì„œ)"""
    import cv2

    if ',' in base64_image:
        base64_image = base64_image.split(',')[1]

    image_bytes = base64.b64decode(base64_image)
    nparr = np.frombuffer(image_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

    if img is None:
        raise ValueError("Failed to decode image")

    if img.shape[:2] != (224, 224):
        img = cv2.resize(img, (224, 224), interpolation=cv2.INTER_LINEAR)

    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return img  # uint8 [224, 224, 3] - ì •ê·œí™”ëŠ” GPUì—ì„œ

def run_batch_inference(jobs: List[InferenceJob]) -> List[Dict[str, Any]]:
    """ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰ - ìµœì í™” ë²„ì „ (CUDA Streams + Pinned Memory + torch.compile)"""
    global compiled_model, device, gpu_mean, gpu_std, inference_stream, transfer_stream

    if not jobs:
        return []

    batch_size = len(jobs)

    # ===== ìµœì í™”: NumPy ì—°ì‚° ë²¡í„°í™” =====
    # ëª¨ë“  í”„ë ˆì„ì„ í•œ ë²ˆì— ìŠ¤íƒ (ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ ëŒ€ì‹ )
    all_frames = []
    for job in jobs:
        # [30, 224, 224, 3] â†’ [3, 30, 224, 224]
        frames_array = np.stack(job.frames, axis=0).transpose(3, 0, 1, 2)
        all_frames.append(frames_array)

    # ë‹¨ì¼ numpy ë°°ì—´ë¡œ í•©ì¹˜ê¸°
    batch_array = np.stack(all_frames, axis=0).astype(np.float32)  # [B, 3, 30, 224, 224]
    batch_array /= 255.0  # CPUì—ì„œ ìŠ¤ì¼€ì¼ë§ (GPU ì—°ì‚° ì¤„ì´ê¸°)

    # ===== ìµœì í™”: CUDA Streamìœ¼ë¡œ ë¹„ë™ê¸° ì „ì†¡ =====
    if transfer_stream is not None:
        with torch.cuda.stream(transfer_stream):
            # Pinned memory â†’ GPU ë¹„ë™ê¸° ì „ì†¡
            input_tensor = torch.from_numpy(batch_array).pin_memory().to(device, non_blocking=True)
            # ì •ê·œí™” (GPUì—ì„œ)
            input_tensor = (input_tensor - gpu_mean.unsqueeze(2)) / gpu_std.unsqueeze(2)

        # ì¶”ë¡  ìŠ¤íŠ¸ë¦¼ì—ì„œ ì „ì†¡ ì™„ë£Œ ëŒ€ê¸° í›„ ì¶”ë¡ 
        if inference_stream is not None:
            inference_stream.wait_stream(transfer_stream)
    else:
        # Fallback: ë™ê¸° ì „ì†¡
        input_tensor = torch.from_numpy(batch_array).to(device, dtype=torch.float32)
        input_tensor = (input_tensor - gpu_mean.unsqueeze(2)) / gpu_std.unsqueeze(2)

    # ===== ìµœì í™”: Mixed Precision + ì¶”ë¡  ìŠ¤íŠ¸ë¦¼ =====
    if inference_stream is not None:
        with torch.cuda.stream(inference_stream):
            with torch.no_grad():
                with torch.cuda.amp.autocast(dtype=torch.float16):  # Mixed Precision
                    output = compiled_model(input_tensor)
                    probabilities = torch.softmax(output, dim=1)
                predicted_classes = torch.argmax(probabilities, dim=1)
        # ê²°ê³¼ ë™ê¸°í™”
        inference_stream.synchronize()
    else:
        with torch.no_grad():
            with torch.cuda.amp.autocast(dtype=torch.float16):  # Mixed Precision
                output = compiled_model(input_tensor)
                probabilities = torch.softmax(output, dim=1)
            predicted_classes = torch.argmax(probabilities, dim=1)

    # ===== ê²°ê³¼ ì¶”ì¶œ (GPUâ†’CPUëŠ” í•œ ë²ˆì—) =====
    pred_classes_cpu = predicted_classes.cpu().numpy()
    probs_cpu = probabilities.cpu().numpy()

    results = []
    for i in range(batch_size):
        pred_class = int(pred_classes_cpu[i])
        confidence = float(probs_cpu[i][pred_class])
        results.append({
            "class_id": pred_class,
            "class_name": CLASS_NAMES[pred_class],
            "confidence": round(confidence * 100, 2),
            "probabilities": {
                CLASS_NAMES[j]: round(float(probs_cpu[i][j]) * 100, 2)
                for j in range(5)
            }
        })

    return results

# GPU ìƒì‹œ ëŒ€ê¸° ë°°ì¹˜ ì›Œì»¤
def gpu_batch_worker():
    """GPUì—ì„œ ìƒì‹œ ëŒ€ê¸°í•˜ë©° ìš”ì²­ ì¦‰ì‹œ ë°°ì¹˜ ì²˜ë¦¬"""
    print("GPU batch worker started - waiting for requests...", flush=True)

    while True:
        jobs = []
        start_time = time.time()

        # ì²« ë²ˆì§¸ ì‘ì—… ëŒ€ê¸° (ë¸”ë¡œí‚¹)
        try:
            first_job = inference_queue.get(timeout=1.0)
            jobs.append(first_job)
            print(f"ğŸ“¥ Got first job for session {first_job.session_id[:8]}, queue size now: {inference_queue.qsize()}", flush=True)
        except Empty:
            continue  # íƒ€ì„ì•„ì›ƒ, ë‹¤ì‹œ ëŒ€ê¸°

        # ë°°ì¹˜ ì±„ìš°ê¸° (BATCH_TIMEOUT ë‚´ì— ë” ëª¨ìœ¼ê¸°)
        while len(jobs) < BATCH_SIZE:
            elapsed = time.time() - start_time
            remaining = BATCH_TIMEOUT - elapsed

            if remaining <= 0:
                break

            try:
                job = inference_queue.get(timeout=remaining)
                jobs.append(job)
            except Empty:
                break

        # ë°°ì¹˜ ì¶”ë¡  ì‹¤í–‰
        print(f"ğŸ”„ Processing batch of {len(jobs)} jobs...", flush=True)
        if jobs:
            try:
                inference_start = time.time()
                results = run_batch_inference(jobs)
                inference_time = (time.time() - inference_start) * 1000
                print(f"âœ… Batch inference complete in {inference_time:.0f}ms", flush=True)

                # ì¶”ë¡  ê²°ê³¼ ë¡œê·¸ ì¶œë ¥
                for i, result in enumerate(results):
                    print(f"ğŸ¯ ì¶”ë¡ ê²°ê³¼ [{jobs[i].session_id[:8]}] {result['class_name']} ({result['confidence']:.1f}%) | ë°°ì¹˜:{len(jobs)} | {inference_time:.0f}ms")

                # ===== ìµœì í™”: ë””ë²„ê·¸ ì €ì¥ ë¹„ë™ê¸° ì²˜ë¦¬ (ì¶”ë¡  ë¸”ë¡œí‚¹ ë°©ì§€) =====
                if SAVE_DEBUG_FRAMES:
                    def save_debug_frames_async(jobs_copy, results_copy, inference_time_copy):
                        """ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ë””ë²„ê·¸ í”„ë ˆì„ ì €ì¥"""
                        import cv2
                        for i, result in enumerate(results_copy):
                            timestamp_str = time.strftime("%H%M%S")
                            session_short = jobs_copy[i].session_id[:8]
                            frames = jobs_copy[i].frames

                            # ì²«ë²ˆì§¸, ì¤‘ê°„, ë§ˆì§€ë§‰ í”„ë ˆì„ ì €ì¥
                            for frame_idx in [0, 14, 29]:
                                if frame_idx < len(frames):
                                    frame = frames[frame_idx]
                                    frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)
                                    filename = f"{DEBUG_DIR}/{timestamp_str}_{session_short}_f{frame_idx}_{result['class_name']}.jpg"
                                    cv2.imwrite(filename, frame_bgr)

                            # ì¶”ë¡  ê¸°ë¡ ì €ì¥
                            inference_record = {
                                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
                                "session_id": session_short,
                                "class_id": result['class_id'],
                                "class_name": result['class_name'],
                                "confidence": result['confidence'],
                                "inference_time_ms": round(inference_time_copy, 1),
                                "frame_count": len(frames),
                                "frame_shape": str(frames[0].shape) if frames else "N/A"
                            }
                            inference_history.append(inference_record)
                            if len(inference_history) > 100:
                                inference_history.pop(0)

                    # ë¹„ë™ê¸° ì‹¤í–‰ (ì¶”ë¡  ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹ ì—†ìŒ)
                    debug_executor.submit(save_debug_frames_async, jobs.copy(), results.copy(), inference_time)

                # ê²°ê³¼ ì „ì†¡
                for i, job in enumerate(jobs):
                    result_data = {
                        "status": "inference_complete",
                        "session_id": job.session_id,
                        "result": results[i],
                        "batch_size": len(jobs),
                        "latency_ms": round((time.time() - job.timestamp) * 1000, 1)
                    }

                    # WebSocketìœ¼ë¡œ ì „ì†¡ (ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ì‚¬ìš©)
                    if job.websocket and main_event_loop:
                        try:
                            # WebSocket ì—°ê²° ìƒíƒœ í™•ì¸
                            if hasattr(job.websocket, 'client_state'):
                                from starlette.websockets import WebSocketState
                                if job.websocket.client_state != WebSocketState.CONNECTED:
                                    print(f"âš ï¸ WebSocket not connected [{job.session_id[:8]}], skipping send")
                                    results_store[job.session_id] = result_data
                                    continue

                            asyncio.run_coroutine_threadsafe(
                                job.websocket.send_json(result_data),
                                main_event_loop
                            ).result(timeout=2.0)  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
                        except Exception as ws_error:
                            print(f"âš ï¸ WebSocket send failed [{job.session_id[:8]}]: {type(ws_error).__name__}")
                            # ì „ì†¡ ì‹¤íŒ¨ ì‹œ HTTP í´ë§ìš©ìœ¼ë¡œ ì €ì¥
                            results_store[job.session_id] = result_data

                    # HTTP í´ë§ìš© ì €ì¥ (í•­ìƒ ì €ì¥)
                    results_store[job.session_id] = result_data

            except Exception as e:
                import traceback
                print(f"âŒ Batch inference error: {e}", flush=True)
                traceback.print_exc()

# í”„ë ˆì„ ìˆ˜ì§‘ ë° í ì¶”ê°€
def add_frame_to_session(session_id: str, frame: np.ndarray, websocket=None):
    """í”„ë ˆì„ ì¶”ê°€ ë° 60í”„ë ˆì„ ë²„í¼ì—ì„œ ìµœì‹  30í”„ë ˆì„ìœ¼ë¡œ ì¶”ë¡ """
    with sessions_lock:
        if session_id not in user_sessions:
            user_sessions[session_id] = {
                'frames': [],
                'last_active': time.time(),
                'websocket': websocket
            }

        session = user_sessions[session_id]
        session['frames'].append(frame)
        session['last_active'] = time.time()
        if websocket:
            session['websocket'] = websocket

        buffer_size = len(session['frames'])

        # 60í”„ë ˆì„ ë²„í¼ê°€ ì°¨ë©´ ìµœì‹  30í”„ë ˆì„ìœ¼ë¡œ ì¶”ë¡ 
        if buffer_size >= FRAME_BUFFER_SIZE:
            job = InferenceJob(
                session_id=session_id,
                frames=session['frames'][-FRAMES_PER_INFERENCE:],  # ìµœì‹  30í”„ë ˆì„
                websocket=session.get('websocket'),
                timestamp=time.time()
            )
            inference_queue.put(job)

            # 10í”„ë ˆì„ ì‹œí”„íŠ¸ (ì¶”ë¡ ë‹¹ 33% ìƒˆ ë°ì´í„°)
            session['frames'] = session['frames'][FRAME_SHIFT:]

            return {
                "status": "queued",
                "buffer_size": len(session['frames']),
                "queue_size": inference_queue.qsize()
            }

        return {
            "status": "buffering",
            "buffer_size": buffer_size,
            "frames_needed": FRAMES_PER_INFERENCE - buffer_size
        }

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘"""
    global main_event_loop

    # ë©”ì¸ ì´ë²¤íŠ¸ ë£¨í”„ ì €ì¥ (ìŠ¤ë ˆë“œê°„ í†µì‹ ìš©)
    main_event_loop = asyncio.get_event_loop()

    init_database()
    load_model()

    # GPU ë°°ì¹˜ ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
    worker_thread = threading.Thread(target=gpu_batch_worker, daemon=True)
    worker_thread.start()

@app.get("/health")
async def health_check():
    with sessions_lock:
        active_sessions = len(user_sessions)
        total_frames = sum(len(s['frames']) for s in user_sessions.values())

    return {
        "status": "healthy",
        "model_loaded": model is not None,
        "device": str(device),
        "active_sessions": active_sessions,
        "total_buffered_frames": total_frames,
        "inference_queue_size": inference_queue.qsize(),
        "batch_size": BATCH_SIZE,
        "batch_timeout_ms": BATCH_TIMEOUT * 1000
    }

# ==================== ë””ë²„ê·¸ API ====================

@app.get("/debug/inference")
async def debug_inference():
    """ìµœê·¼ ì¶”ë¡  ê¸°ë¡ ì¡°íšŒ"""
    import glob

    # ì €ì¥ëœ ë””ë²„ê·¸ ì´ë¯¸ì§€ ëª©ë¡
    debug_images = sorted(glob.glob(f"{DEBUG_DIR}/*.jpg"), key=os.path.getmtime, reverse=True)[:30]
    image_files = [os.path.basename(f) for f in debug_images]

    return {
        "total_inferences": len(inference_history),
        "recent_inferences": inference_history[-20:],  # ìµœê·¼ 20ê°œ
        "debug_images": image_files,
        "debug_dir": DEBUG_DIR,
        "save_enabled": SAVE_DEBUG_FRAMES
    }

@app.get("/debug/frame/{filename}")
async def get_debug_frame(filename: str):
    """ë””ë²„ê·¸ í”„ë ˆì„ ì´ë¯¸ì§€ ì¡°íšŒ"""
    file_path = os.path.join(DEBUG_DIR, filename)
    if os.path.exists(file_path):
        return FileResponse(file_path, media_type="image/jpeg")
    raise HTTPException(status_code=404, detail="Frame not found")

@app.delete("/debug/clear")
async def clear_debug():
    """ë””ë²„ê·¸ ë°ì´í„° ì´ˆê¸°í™”"""
    import glob
    for f in glob.glob(f"{DEBUG_DIR}/*.jpg"):
        os.remove(f)
    inference_history.clear()
    return {"status": "cleared", "message": "Debug data cleared"}

# ==================== ì¸ì¦ API ====================

@app.post("/auth/signup")
async def signup(request: SignUpRequest):
    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        cursor.execute('SELECT id FROM users WHERE id = ?', (request.id,))
        if cursor.fetchone():
            conn.close()
            raise HTTPException(status_code=400, detail="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì•„ì´ë””ì…ë‹ˆë‹¤")

        cursor.execute('''
            INSERT INTO users (id, name, password, address, region_name, region_campaign,
                             region_target, region_reward, score, discount_rate, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            request.id, request.name, request.password, request.address,
            request.region_name, request.region_campaign, request.region_target,
            request.region_reward, 80, 0, time.strftime('%Y-%m-%d %H:%M:%S')
        ))

        conn.commit()
        conn.close()
        return {"success": True, "message": "íšŒì›ê°€ì…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤"}

    except sqlite3.Error as e:
        conn.close()
        raise HTTPException(status_code=500, detail=f"DB ì˜¤ë¥˜: {str(e)}")

@app.post("/auth/login")
async def login(request: LoginRequest):
    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        cursor.execute('SELECT * FROM users WHERE id = ?', (request.id,))
        user = cursor.fetchone()
        conn.close()

        if not user or user['password'] != request.password:
            raise HTTPException(status_code=401, detail="ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ í‹€ë ¸ìŠµë‹ˆë‹¤")

        return {
            "success": True,
            "user": {
                "id": user['id'],
                "name": user['name'],
                "score": user['score'],
                "discount_rate": user['discount_rate'],
                "region": {
                    "name": user['region_name'],
                    "campaign": user['region_campaign'],
                    "target": user['region_target'],
                    "reward": user['region_reward'],
                    "address": user['address']
                }
            }
        }

    except sqlite3.Error as e:
        raise HTTPException(status_code=500, detail=f"DB ì˜¤ë¥˜: {str(e)}")

@app.get("/auth/user/{user_id}")
async def get_user(user_id: str):
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute('SELECT * FROM users WHERE id = ?', (user_id,))
    user = cursor.fetchone()
    conn.close()

    if not user:
        raise HTTPException(status_code=404, detail="ì‚¬ìš©ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    return {
        "id": user['id'],
        "name": user['name'],
        "score": user['score'],
        "discount_rate": user['discount_rate'],
        "region": {
            "name": user['region_name'],
            "campaign": user['region_campaign'],
            "target": user['region_target'],
            "reward": user['region_reward'],
            "address": user['address']
        }
    }

# ==================== ìš´ì „ ê¸°ë¡ API ====================

@app.post("/driving/start")
async def start_driving(request: StartDrivingRequest):
    """ìš´ì „ ì‹œì‘ - ìƒˆ ìš´ì „ ê¸°ë¡ ìƒì„±"""
    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        start_time = time.strftime('%Y-%m-%d %H:%M:%S')
        cursor.execute('''
            INSERT INTO driving_logs (user_id, start_time, status)
            VALUES (?, ?, 'driving')
        ''', (request.user_id, start_time))

        conn.commit()
        driving_log_id = cursor.lastrowid
        conn.close()

        return {
            "success": True,
            "driving_log_id": driving_log_id,
            "start_time": start_time
        }

    except sqlite3.Error as e:
        conn.close()
        raise HTTPException(status_code=500, detail=f"DB ì˜¤ë¥˜: {str(e)}")

@app.post("/driving/end")
async def end_driving(request: EndDrivingRequest):
    """ìš´ì „ ì¢…ë£Œ - ìš´ì „ ê¸°ë¡ ì—…ë°ì´íŠ¸"""
    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        end_time = time.strftime('%Y-%m-%d %H:%M:%S')
        cursor.execute('''
            UPDATE driving_logs
            SET end_time = ?, status = 'completed'
            WHERE id = ?
        ''', (end_time, request.driving_log_id))

        conn.commit()
        conn.close()

        return {
            "success": True,
            "driving_log_id": request.driving_log_id,
            "end_time": end_time
        }

    except sqlite3.Error as e:
        conn.close()
        raise HTTPException(status_code=500, detail=f"DB ì˜¤ë¥˜: {str(e)}")

@app.post("/driving/detection")
async def save_detection(request: SaveDetectionRequest):
    """ëª¨ë¸ ê°ì§€ ê²°ê³¼ ì €ì¥"""
    conn = get_db_connection()
    cursor = conn.cursor()

    try:
        detected_at = time.strftime('%Y-%m-%d %H:%M:%S')

        # ê°ì§€ ê¸°ë¡ ì €ì¥
        cursor.execute('''
            INSERT INTO driving_detections (driving_log_id, detected_at, class_id, class_name, confidence)
            VALUES (?, ?, ?, ?, ?)
        ''', (request.driving_log_id, detected_at, request.class_id, request.class_name, request.confidence))

        # ìš´ì „ ê¸°ë¡ì˜ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
        count_column = {
            0: 'normal_count',
            1: 'drowsy_count',
            2: 'searching_count',
            3: 'phone_count',
            4: 'assault_count'
        }.get(request.class_id, 'normal_count')

        cursor.execute(f'''
            UPDATE driving_logs
            SET total_detections = total_detections + 1,
                {count_column} = {count_column} + 1
            WHERE id = ?
        ''', (request.driving_log_id,))

        conn.commit()
        conn.close()

        return {"success": True, "detected_at": detected_at}

    except sqlite3.Error as e:
        conn.close()
        raise HTTPException(status_code=500, detail=f"DB ì˜¤ë¥˜: {str(e)}")

@app.get("/driving/logs/{user_id}")
async def get_driving_logs(user_id: str):
    """ì‚¬ìš©ìì˜ ëª¨ë“  ìš´ì „ ê¸°ë¡ ì¡°íšŒ"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute('''
        SELECT * FROM driving_logs
        WHERE user_id = ?
        ORDER BY start_time DESC
    ''', (user_id,))

    logs = cursor.fetchall()
    conn.close()

    return {
        "logs": [
            {
                "id": log['id'],
                "start_time": log['start_time'],
                "end_time": log['end_time'],
                "status": log['status'],
                "total_detections": log['total_detections'],
                "normal_count": log['normal_count'],
                "drowsy_count": log['drowsy_count'],
                "searching_count": log['searching_count'],
                "phone_count": log['phone_count'],
                "assault_count": log['assault_count']
            }
            for log in logs
        ]
    }

@app.get("/driving/log/{driving_log_id}")
async def get_driving_log_detail(driving_log_id: int):
    """íŠ¹ì • ìš´ì „ ê¸°ë¡ ìƒì„¸ ì¡°íšŒ (ê°ì§€ ê¸°ë¡ í¬í•¨)"""
    conn = get_db_connection()
    cursor = conn.cursor()

    # ìš´ì „ ê¸°ë¡ ì¡°íšŒ
    cursor.execute('SELECT * FROM driving_logs WHERE id = ?', (driving_log_id,))
    log = cursor.fetchone()

    if not log:
        conn.close()
        raise HTTPException(status_code=404, detail="ìš´ì „ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    # ê°ì§€ ê¸°ë¡ ì¡°íšŒ
    cursor.execute('''
        SELECT * FROM driving_detections
        WHERE driving_log_id = ?
        ORDER BY detected_at ASC
    ''', (driving_log_id,))

    detections = cursor.fetchall()
    conn.close()

    return {
        "log": {
            "id": log['id'],
            "user_id": log['user_id'],
            "start_time": log['start_time'],
            "end_time": log['end_time'],
            "status": log['status'],
            "total_detections": log['total_detections'],
            "normal_count": log['normal_count'],
            "drowsy_count": log['drowsy_count'],
            "searching_count": log['searching_count'],
            "phone_count": log['phone_count'],
            "assault_count": log['assault_count']
        },
        "detections": [
            {
                "id": det['id'],
                "detected_at": det['detected_at'],
                "class_id": det['class_id'],
                "class_name": det['class_name'],
                "confidence": det['confidence']
            }
            for det in detections
        ]
    }

# ==================== ì„¸ì…˜/ì¶”ë¡  API ====================

@app.post("/session/create")
async def create_session():
    session_id = str(uuid.uuid4())
    with sessions_lock:
        user_sessions[session_id] = {
            'frames': [],
            'last_active': time.time(),
            'websocket': None
        }
    return {"session_id": session_id}

@app.delete("/session/{session_id}")
async def delete_session(session_id: str):
    with sessions_lock:
        if session_id in user_sessions:
            del user_sessions[session_id]
            return {"status": "deleted"}
    raise HTTPException(status_code=404, detail="Session not found")

@app.post("/infer")
async def infer(request: InferRequest):
    """í”„ë ˆì„ ì¶”ê°€ - 30í”„ë ˆì„ ë„ë‹¬ì‹œ ì¦‰ì‹œ íì— ì¶”ê°€"""
    try:
        frame = preprocess_image(request.image)
        result = add_frame_to_session(request.session_id, frame)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/result/{session_id}")
async def get_result(session_id: str):
    """ì¶”ë¡  ê²°ê³¼ í´ë§ (HTTPìš©)"""
    if session_id in results_store:
        return results_store.pop(session_id)
    return {"status": "pending"}

@app.websocket("/ws/{session_id}")
async def websocket_endpoint(websocket: WebSocket, session_id: str):
    """WebSocket ì‹¤ì‹œê°„ í†µì‹  - keep-alive ê°•í™”"""
    await websocket.accept()
    print(f"âœ… WebSocket connected: {session_id[:8]}")

    with sessions_lock:
        if session_id not in user_sessions:
            user_sessions[session_id] = {
                'frames': [],
                'last_active': time.time(),
                'websocket': websocket
            }
        else:
            user_sessions[session_id]['websocket'] = websocket

    # ì„œë²„ ì¸¡ ping íƒœìŠ¤í¬ (keep-alive) - RunPod í”„ë¡ì‹œ íƒ€ì„ì•„ì›ƒ ë°©ì§€
    async def server_ping():
        """2ì´ˆë§ˆë‹¤ ping ì „ì†¡í•˜ì—¬ í”„ë¡ì‹œ íƒ€ì„ì•„ì›ƒ ë°©ì§€ (ì¦‰ì‹œ ì‹œì‘)"""
        ping_count = 0
        try:
            # ì¦‰ì‹œ ì²« ping ì „ì†¡ (ì—°ê²° ì§í›„)
            await websocket.send_json({"type": "server_ping", "timestamp": time.time()})
            ping_count += 1
            print(f"ğŸ“ Ping #{ping_count} sent [{session_id[:8]}]")

            while True:
                await asyncio.sleep(2)  # 5ì´ˆ -> 2ì´ˆë¡œ ë‹¨ì¶•
                try:
                    await websocket.send_json({"type": "server_ping", "timestamp": time.time()})
                    ping_count += 1
                    if ping_count % 10 == 0:  # 20ì´ˆë§ˆë‹¤ ë¡œê·¸
                        print(f"ğŸ“ Ping #{ping_count} sent [{session_id[:8]}]")
                except Exception as e:
                    print(f"âš ï¸ Ping failed [{session_id[:8]}]: {e}")
                    break
        except asyncio.CancelledError:
            pass

    ping_task = asyncio.create_task(server_ping())

    try:
        while True:
            try:
                # íƒ€ì„ì•„ì›ƒ ì„¤ì •ìœ¼ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
                data = await asyncio.wait_for(websocket.receive_json(), timeout=30.0)
            except asyncio.TimeoutError:
                # 30ì´ˆê°„ ë°ì´í„° ì—†ìœ¼ë©´ ping ì „ì†¡
                await websocket.send_json({"type": "server_ping", "timestamp": time.time()})
                continue

            if data.get('type') == 'frame':
                try:
                    frame = preprocess_image(data['image'])
                    result = add_frame_to_session(session_id, frame, websocket)
                    await websocket.send_json(result)
                except Exception as e:
                    print(f"âš ï¸ Frame processing error [{session_id[:8]}]: {e}")
                    await websocket.send_json({"status": "error", "message": str(e)})

            elif data.get('type') == 'ping':
                await websocket.send_json({"type": "pong", "timestamp": time.time()})

            elif data.get('type') == 'pong':
                # í´ë¼ì´ì–¸íŠ¸ pong ì‘ë‹µ ë¬´ì‹œ (keep-alive í™•ì¸ìš©)
                pass

    except WebSocketDisconnect:
        print(f"ğŸ”Œ WebSocket disconnected (client): {session_id[:8]}")
    except Exception as e:
        print(f"âŒ WebSocket error [{session_id[:8]}]: {type(e).__name__}: {e}")
    finally:
        ping_task.cancel()
        with sessions_lock:
            if session_id in user_sessions:
                user_sessions[session_id]['websocket'] = None
        print(f"ğŸ”š WebSocket cleanup done: {session_id[:8]}")

# ì„¸ì…˜ ì •ë¦¬
async def cleanup_sessions():
    while True:
        await asyncio.sleep(60)
        current_time = time.time()
        with sessions_lock:
            expired = [sid for sid, s in user_sessions.items() if current_time - s['last_active'] > 300]
            for sid in expired:
                del user_sessions[sid]
                if sid in results_store:
                    del results_store[sid]

@app.on_event("startup")
async def start_cleanup():
    asyncio.create_task(cleanup_sessions())

# ==================== í”„ë¡ íŠ¸ì—”ë“œ ì •ì  íŒŒì¼ ì„œë¹™ ====================

FRONTEND_DIR = "/workspace/ai-2026-c-team/driver_front/dist"

# ì •ì  íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë§ˆìš´íŠ¸
if os.path.exists(FRONTEND_DIR):
    app.mount("/assets", StaticFiles(directory=f"{FRONTEND_DIR}/assets"), name="assets")

    @app.get("/{full_path:path}")
    async def serve_spa(full_path: str):
        """SPA fallback - ëª¨ë“  ê²½ë¡œë¥¼ index.htmlë¡œ"""
        # API ê²½ë¡œëŠ” ì œì™¸
        if full_path.startswith(("auth/", "driving/", "session/", "ws/", "health", "infer", "result/")):
            raise HTTPException(status_code=404)

        # ì •ì  íŒŒì¼ í™•ì¸
        file_path = os.path.join(FRONTEND_DIR, full_path)
        if os.path.isfile(file_path):
            return FileResponse(file_path)

        # SPA fallback
        return FileResponse(f"{FRONTEND_DIR}/index.html")

if __name__ == "__main__":
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,  # í”„ë¡ íŠ¸ì—”ë“œ í”„ë¡ì‹œì™€ ì¼ì¹˜
        ws_ping_interval=10,   # 20 -> 10ì´ˆ (ë” ë¹ ë¥¸ ping)
        ws_ping_timeout=20,    # 30 -> 20ì´ˆ (ë” ë¹ ë¥¸ ê°ì§€)
        timeout_keep_alive=300 # 120 -> 300ì´ˆ (keep-alive ì—°ì¥)
    )
